{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import datasets\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set, test_set = datasets.load_SVHN()\n",
    "train_x, train_y = train_set\n",
    "test_x, test_y = test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def batches(X,y,batch_size=128):\n",
    "#     assert len(X) == len(y)\n",
    "#     n = len(X)\n",
    "#     p = np.random.permutation(n)\n",
    "    \n",
    "#     num_batches = n // batch_size\n",
    "#     for i in range(num_batches):\n",
    "#         start = i*batch_size\n",
    "#         end = start+batch_size\n",
    "#         yield X[p[start:end]], y[p[start:end]]\n",
    "\n",
    "#     left_over = n % batch_size\n",
    "#     yield X[p[-left_over:]], y[p[-left_over:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_x[0].reshape(32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    \"\"\" Xavier initialization of network weights\"\"\"\n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval=low, maxval=high, \n",
    "                             dtype=tf.float32)\n",
    "\n",
    "#renaming functions for convinience\n",
    "dropout = tf.nn.dropout\n",
    "sigmoid = tf.nn.sigmoid\n",
    "ReLu = tf.nn.relu\n",
    "elu  = tf.nn.elu\n",
    "conv = tf.nn.conv2d\n",
    "deconv = tf.nn.conv2d_transpose\n",
    "clip = tf.clip_by_value\n",
    "Adam = tf.train.AdamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(object):\n",
    "    \"\"\" Variation Autoencoder (VAE) with an sklearn-like interface implemented using TensorFlow.\n",
    "    \n",
    "    This implementation uses probabilistic encoders and decoders using Gaussian \n",
    "    distributions and  realized by multi-layer perceptrons. The VAE can be learned\n",
    "    end-to-end.\n",
    "    \n",
    "    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self, network_architecture, learning_rate=0.001, batch_size=100):\n",
    "        \n",
    "        self.network_architecture = network_architecture\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # tf Graph input\n",
    "        n_input = network_architecture[\"n_input\"]\n",
    "        self.x = tf.placeholder(tf.float32, [None, n_input],'x_placeholder')\n",
    "        \n",
    "        #Adversarial part of the network\n",
    "        self.attack = tf.Variable(tf.zeros([n_input], dtype=tf.float32),name='Attack')\n",
    "        self.adv_x = clip(self.x + self.attack, clip_value_min=0, clip_value_max=1, name='adversarial_x')\n",
    "        \n",
    "        n_z = network_architecture[\"n_z\"]\n",
    "        self.target = tf.placeholder(tf.float32, [None, n_z],'target_z')\n",
    "        self.C = tf.Variable(1, dtype=tf.float32, trainable=False,name='C')\n",
    "        \n",
    "        self.adv_im = tf.reshape(self.adv_x, shape=[-1, 32, 32, 3])\n",
    "        \n",
    "        # Create autoencoder network\n",
    "        self._create_network()\n",
    "        \n",
    "        \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        \n",
    "        # Define loss function based variational upper-bound and \n",
    "        # corresponding optimizer\n",
    "        self._create_loss_optimizer()\n",
    "        \n",
    "        # Initializing the tensor flow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Launch the session\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def _create_network(self):\n",
    "        # create keepprob placeholder\n",
    "        self.keepprob = tf.placeholder(tf.float32,name='keepprob')\n",
    "        \n",
    "        # creates autoencode network weights and biases\n",
    "        weights = self._initialize_weights(**self.network_architecture)\n",
    "        self.weights = weights\n",
    "        \n",
    "        # Use recognition network to determine mean and \n",
    "        # (log) variance of Gaussian distribution in latent\n",
    "        # space\n",
    "        self.z_mean, self.z_log_sigma_sq = \\\n",
    "            self._recognition_network(weights[\"weights_recog\"], \n",
    "                                      weights[\"biases_recog\"])\n",
    "\n",
    "        # Draw one sample z from Gaussian distribution\n",
    "        n_z = self.network_architecture[\"n_z\"]\n",
    "        eps = tf.random_normal((self.batch_size, n_z), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        # z = mu + sigma*epsilon\n",
    "        self.z = self.z_mean + eps*tf.sqrt(tf.exp(self.z_log_sigma_sq))\n",
    "\n",
    "        # Use generator to determine mean of\n",
    "        # Bernoulli distribution of reconstructed input\n",
    "        self.x_reconstr_mean = \\\n",
    "            self._generator_network(weights[\"weights_gener\"],\n",
    "                                    weights[\"biases_gener\"])\n",
    "            \n",
    "    def _initialize_weights(self, n_input, ksize, n1,  n2, n3, n_z):\n",
    "        all_weights = dict()\n",
    "        all_weights['weights_recog'] = {\n",
    "            'ce1': tf.Variable(tf.random_normal([ksize, ksize, 3, n1],  stddev=0.1),name='ce1'),\n",
    "            'ce2': tf.Variable(tf.random_normal([ksize, ksize, n1, n2], stddev=0.1),name='ce2'),\n",
    "            'ce3': tf.Variable(tf.random_normal([ksize, ksize, n2, n3], stddev=0.1),name='ce3'),\n",
    "            'mean': tf.Variable(xavier_init(n3*4*4, n_z),name='w_mean'),\n",
    "            'log_sigma': tf.Variable(xavier_init(n3*4*4, n_z),name='w_log_sigma')}\n",
    "        all_weights['biases_recog'] = {\n",
    "            'be1': tf.Variable(tf.random_normal([n1], stddev=0.1),name='be1'),\n",
    "            'be2': tf.Variable(tf.random_normal([n2], stddev=0.1),name='be2'),\n",
    "            'be3': tf.Variable(tf.random_normal([n3], stddev=0.1),name='be3'),\n",
    "            'mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32),name='b_mean'),\n",
    "            'log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32),name='b_log_sigma')}\n",
    "        \n",
    "        all_weights['weights_gener'] = {\n",
    "            'from_z':tf.Variable(xavier_init(n_z,n3*4*4),name='from_z'),\n",
    "            'cd3': tf.Variable(tf.random_normal([ksize, ksize, n2, n3], stddev=0.1),name='cd3'),\n",
    "            'cd2': tf.Variable(tf.random_normal([ksize, ksize, n1, n2], stddev=0.1),name='cd2'),\n",
    "            'cd1': tf.Variable(tf.random_normal([ksize, ksize, 3, n1],  stddev=0.1),name='cd1'),}\n",
    "        all_weights['biases_gener'] = {\n",
    "            'from_z': tf.Variable(tf.zeros([n3*4*4], dtype=tf.float32),name='b_from_z'),\n",
    "            'bd3': tf.Variable(tf.random_normal([n2], stddev=0.1),name='bd3'),\n",
    "            'bd2': tf.Variable(tf.random_normal([n1], stddev=0.1),name='bd2'),\n",
    "            'bd1': tf.Variable(tf.random_normal([3],  stddev=0.1),name='bd1'),}\n",
    "        return all_weights\n",
    "            \n",
    "    def _recognition_network(self, w, b):\n",
    "        # Generate probabilistic encoder (recognition network), which\n",
    "        # maps inputs onto a normal distribution in latent space.\n",
    "        # The transformation is parametrized and can be learned.\n",
    "        pad = 'SAME'\n",
    "        stri = [1, 2, 2, 1]\n",
    "        \n",
    "        ce1 = sigmoid( conv(self.adv_im, w['ce1'], strides=stri, padding=pad) + b['be1'] )\n",
    "        #ce1 = dropout( ce1, self.keepprob ) #image size (16,16,n1)\n",
    "        ce2 = sigmoid( conv(ce1, w['ce2'], strides=stri, padding=pad) + b['be2'] ) \n",
    "        #ce2 = dropout( ce2, self.keepprob ) #image size (8,8,n2)\n",
    "        ce3 = sigmoid( conv(ce2, w['ce3'], strides=stri, padding=pad) + b['be3'] )\n",
    "        #ce3 = dropout( ce3, self.keepprob ) #image size (4,4,n3)\n",
    "        \n",
    "        n3 = self.network_architecture['n3']\n",
    "        flat = tf.reshape(ce3, [-1, 4*4*n3])\n",
    "        \n",
    "        z_mean = tf.matmul(flat, w['mean']) + b['mean']\n",
    "        z_log_sigma_sq = tf.matmul(flat, w['log_sigma']) + b['log_sigma']\n",
    "        \n",
    "        #valores para debugar\n",
    "        self._ce1 = ce1\n",
    "        self._ce2 = ce2\n",
    "        self._ce3 = ce3\n",
    "        self._flat = flat\n",
    "        \n",
    "        return (z_mean, z_log_sigma_sq)\n",
    "\n",
    "    def _generator_network(self, w, b):\n",
    "        # Generate probabilistic decoder (decoder network), which\n",
    "        # maps points in latent space onto a Bernoulli distribution in data space.\n",
    "        # The transformation is parametrized and can be learned.\n",
    "        pad = 'SAME' #padding\n",
    "        stri = [1,2,2,1] #strides\n",
    "        n3 = self.network_architecture[\"n3\"]\n",
    "        n2 = self.network_architecture[\"n2\"]\n",
    "        n1 = self.network_architecture[\"n1\"]\n",
    "        n_ex = tf.shape(self.x)[0] #number of examples\n",
    "        dim3 = tf.stack([n_ex, 8, 8, n2])\n",
    "        dim2 = tf.stack([n_ex, 16, 16, n1])\n",
    "        dim1 = tf.stack([n_ex, 32, 32, 3])\n",
    "        \n",
    "        from_z = elu(tf.matmul(self.z, w['from_z']) + b['from_z'])\n",
    "        im_z = tf.reshape(from_z, [-1,4,4,n3],name='im_z')\n",
    "        \n",
    "        cd3 = sigmoid(deconv(im_z, w['cd3'], dim3, strides=stri, padding=pad,name='deconv3') +  b['bd3']) \n",
    "        #cd3 = dropout(cd3, self.keepprob) #image size (8,8,n2)\n",
    "        cd2 = sigmoid(deconv(cd3, w['cd2'], dim2, strides=stri, padding=pad,name='deconv2') + b['bd2']) \n",
    "        #cd2 = dropout(cd2, self.keepprob) #image size (16,16,n2)\n",
    "        cd1 = sigmoid(deconv(cd2, w['cd1'], dim1, strides=stri, padding=pad,name='deconv1') + b['bd1']) \n",
    "        #cd1 = dropout(cd1, self.keepprob) #iamge size (32,32,3)\n",
    "        \n",
    "        x_reconstr_mean = tf.reshape(cd1, [-1,32*32*3])\n",
    "        \n",
    "        #valores para debugar\n",
    "        self.from_z = from_z\n",
    "        self.im_z = im_z\n",
    "        self.cd1 = cd1\n",
    "        self.cd2 = cd2\n",
    "        self.cd3 = cd3\n",
    "        \n",
    "        return x_reconstr_mean\n",
    "\n",
    "    def _create_loss_optimizer(self):\n",
    "        #getting variables that are not form the attack\n",
    "        all_vars = self.sess.graph.get_collection('trainable_variables')\n",
    "        w = [var for var in all_vars if 'Attack' not in var.name]\n",
    "        \n",
    "        def kl_divergence(mean1, log_var1, mean2, log_var2):\n",
    "            mean_term = (tf.exp(0.5 * log_var1) + tf.pow(mean1 - mean2, 2)) \\\n",
    "                / tf.exp(0.5 * log_var2)\n",
    "            return mean_term + log_var2 - log_var1 - 0.5\n",
    "        \n",
    "        \n",
    "        \n",
    "        # The loss is composed of two terms:\n",
    "        # 1.) The reconstruction loss (the negative log probability\n",
    "        #     of the input under the reconstructed Bernoulli distribution \n",
    "        #     induced by the decoder in the data space).\n",
    "        reconstr_loss = -tf.reduce_sum(\n",
    "            self.x * tf.log(1e-8 + self.x_reconstr_mean) + (1-self.x) * tf.log(1e-8 + 1 - self.x_reconstr_mean),\n",
    "                           1)\n",
    "        \n",
    "        \n",
    "        # 2.) The latent loss, which is defined as the Kullback Leibler divergence \n",
    "        ##    between the distribution in latent space induced by the encoder on \n",
    "        #     the data and some prior. This acts as a kind of regularizer.\n",
    "        latent_loss = -0.5 * tf.reduce_sum(\n",
    "            1 + self.z_log_sigma_sq - tf.square(self.z_mean) - tf.exp(self.z_log_sigma_sq), \n",
    "                                1)\n",
    "        # 3.) The weight loss, (no pun intended), which is a simple\n",
    "        weight_loss = 0\n",
    "        for var in w:\n",
    "            weight_loss += tf.nn.l2_loss(var)\n",
    "        \n",
    "        self.B = tf.placeholder(tf.float32,name='B')\n",
    "        self.G = tf.placeholder(tf.float32,name='G')\n",
    "        self.cost = tf.reduce_mean(reconstr_loss + self.B*latent_loss)   + self.G*weight_loss# average over batch\n",
    "        cost = self.cost\n",
    "        \n",
    "        self.reconstr_loss = tf.reduce_mean(reconstr_loss)\n",
    "        self.latent_loss = tf.reduce_mean(latent_loss)\n",
    "        self.weight_loss = weight_loss\n",
    "        \n",
    "        \n",
    "        # Use ADAM optimizer\n",
    "        self.optimizer = Adam(learning_rate=self.learning_rate).minimize(cost,var_list=w)\n",
    "        \n",
    "        \n",
    "        self.target_cost = tf.square(self.target - self.z_mean) + self.C*tf.nn.l2_loss(self.attack)\n",
    "        target_cost = self.target_cost\n",
    "        \n",
    "        #Adversarial optimizer\n",
    "        self.adv_optimizer = Adam(learning_rate=self.learning_rate).minimize(target_cost, var_list=[self.attack])\n",
    "    \n",
    "    def reset_attack(self):\n",
    "        self.sess.run(tf.variables_initializer([self.attack]))\n",
    "    \n",
    "    def partial_fit(self, X, keepprob, B, G):\n",
    "        \"\"\"Train model based on mini-batch of input data.\n",
    "        \n",
    "        Return cost of mini-batch.\n",
    "        \"\"\"\n",
    "        self.b = B\n",
    "        self.g = G\n",
    "        values = self.sess.run((self.optimizer, self.cost, self.reconstr_loss, self.latent_loss, self.weight_loss),\n",
    "                                  feed_dict={self.x: X, self.keepprob:keepprob,self.B:B,self.G:G})\n",
    "        \n",
    "        opt, cost, recon_loss, lat_loss, w_loss = values\n",
    "        \n",
    "        return cost, recon_loss, lat_loss, w_loss\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data by mapping it into the latent space.\"\"\"\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "        return self.sess.run(self.z_mean, feed_dict={self.x: X,self.keepprob:1,self.B:self.b,self.G:self.g})\n",
    "    \n",
    "    def generate(self, z_mu=None):\n",
    "        \"\"\" Generate data by sampling from latent space.\n",
    "        \n",
    "        If z_mu is not None, data for this point in latent space is\n",
    "        generated. Otherwise, z_mu is drawn from prior in latent \n",
    "        space.        \n",
    "        \"\"\"\n",
    "        if z_mu is None:\n",
    "            z_mu = np.random.normal(size=self.network_architecture[\"n_z\"])\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.z: z_mu,self.keepprob:1,self.B:self.b,self.G:self.g})\n",
    "    \n",
    "    def reconstruct(self, X):\n",
    "        \"\"\" Use VAE to reconstruct given data. \"\"\"\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.x: X, self.keepprob:1,self.B:self.b,self.G:self.g})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kl_term = tf.reduce_sum(kl_divergence(z_mean, z_log_var, 0., 1.), axis=1)\n",
    "log_px_given_z = tf.reduce_sum(log_normal2(tf.reshape(x, [tf.shape(x)[0], -1]), out_mean, out_log_var), axis=1)\n",
    "reconstr_loss = -tf.reduce_mean(-kl_term + log_px_given_z)\n",
    "\n",
    "c = - 0.5 * math.log(2 * math.pi)\n",
    "    \n",
    "def log_normal2(x, mean, log_var, eps=0.0):\n",
    "    return c - log_var/2 - (x - mean)**2 / (2 * tf.exp(log_var) + eps)\n",
    "\n",
    "def kl_divergence(mean1, log_var1, mean2, log_var2):\n",
    "    mean_term = (tf.exp(0.5 * log_var1) + tf.pow(mean1 - mean2, 2)) \\\n",
    "        / tf.exp(0.5 * log_var2)\n",
    "    return mean_term + log_var2 - log_var1 - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(network_architecture, learning_rate=0.001,\n",
    "          batch_size=100, training_epochs=10, display_step=1):\n",
    "    vae = VariationalAutoencoder(network_architecture, \n",
    "                                 learning_rate=learning_rate, \n",
    "                                 batch_size=batch_size)\n",
    "    n_samples = len(train_x)\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        avg_recon_loss = 0.\n",
    "        avg_lat_loss = 0.\n",
    "        avg_w_loss = 0.\n",
    "        # Loop over all batches\n",
    "        for batch_xs, y in datasets.batches(train_x,train_y,batch_size=batch_size):\n",
    "            if len(batch_xs) != batch_size:\n",
    "                continue\n",
    "            # Fit training using batch data\n",
    "            cost, recon_loss, lat_loss, w_loss = vae.partial_fit(batch_xs, 0.5, 0.01, 0.0001)\n",
    "            \n",
    "            # Compute average loss\n",
    "            avg_cost += cost / n_samples * batch_size\n",
    "            avg_recon_loss += recon_loss / n_samples * batch_size\n",
    "            avg_lat_loss += lat_loss / n_samples * batch_size\n",
    "            avg_w_loss += w_loss / n_samples * batch_size\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"EPOCH: %04d, COST: %.5f, RECONSTR_LOSS: %.5f, LATENT_LOSS: %.5f, WEIGHT_LOSS %.5f\" % \n",
    "                   (epoch+1, avg_cost, avg_recon_loss, avg_lat_loss, w_loss))\n",
    "    print (\"EPOCH: %04d, COST: %.5f, RECONSTR_LOSS: %.5f, LATENT_LOSS: %.5f, WEIGHT_LOSS %.5f\" % \n",
    "                   (epoch+1, avg_cost, avg_recon_loss, avg_lat_loss, w_loss))\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "network_architecture = \\\n",
    "    dict(n_input=32*32*3, # SVHN data input (img shape: 32*32*3)\n",
    "         ksize=5, # Size of filters\n",
    "         n1=32, # Filters in 1st layer\n",
    "         n2=64, # Filters in 2nd layer\n",
    "         n3=128, # Filters in 3rd layer\n",
    "         n_z=100)  # dimensionality of latent space\n",
    "\n",
    "vae = train(network_architecture, training_epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_ex = len(train_x)\n",
    "x_sample = train_x[np.random.choice(n_ex,100)]\n",
    "x_reconstruct = vae.reconstruct(x_sample)\n",
    "plt.figure(figsize=(8, 12))\n",
    "for i in range(5):\n",
    "    plt.subplot(5, 2, 2*i + 1)\n",
    "    plt.imshow(x_sample[i].reshape(32, 32,3), vmin=0, vmax=1, cmap=\"gray\")\n",
    "    plt.title(\"Test input\")\n",
    "    plt.subplot(5, 2, 2*i + 2)\n",
    "    plt.imshow(x_reconstruct[i].reshape(32, 32,3), vmin=0, vmax=1, cmap=\"gray\")\n",
    "    plt.title(\"Reconstruction\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(series=[],titles=[],suptitle='',img_dim = (28,28)):\n",
    "    num_colums = 3\n",
    "    num_imgs = len(series)\n",
    "    num_rows = 2\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.suptitle(suptitle,fontsize=16)\n",
    "    for i in range(num_imgs):\n",
    "        plt.subplot(num_rows, num_colums, i+1)\n",
    "        plt.imshow(series[i].reshape(img_dim),vmin=0,vmax=1,cmap='gray')\n",
    "        plt.title(titles[i])\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = len(x_sample)\n",
    "i_original = np.random.choice(n)\n",
    "i_target = np.random.choice(n)\n",
    "\n",
    "original = np.array([x_sample[i_original]])\n",
    "target = np.array([x_sample[i_target]])\n",
    "\n",
    "series = [original,target]\n",
    "titles = ['original','target']\n",
    "plot_images(series=series,titles=titles, img_dim=(32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([10000.],np.float32)*a\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vae.sess.run(tf.variables_initializer([vae.attack]))\n",
    "z_target = vae.transform(target)\n",
    "original_reconstruction = vae.reconstruct(original)[0]\n",
    "original_target_reconstruction = vae.reconstruct(target)[0]\n",
    "\n",
    "distance = np.linalg.norm\n",
    "\n",
    "\n",
    "dist_target_rec_target = distance(target[0]-original_target_reconstruction)\n",
    "dist_target_rec_original = distance(target[0]-original_reconstruction)\n",
    "dist_target_original = distance(original_target_reconstruction-original_reconstruction)\n",
    "\n",
    "vae.sess.run(tf.variables_initializer([vae.attack]))\n",
    "attack = vae.sess.run(vae.attack)\n",
    "size_attack = np.linalg.norm(attack)\n",
    "\n",
    "point = (size_attack,dist_target_rec_original)\n",
    "\n",
    "points = [point]\n",
    "\n",
    "feed_dict={vae.x:original,vae.target:z_target,vae.B:0.,vae.G:vae.g}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vae.sess.run(tf.variables_initializer([vae.attack]))\n",
    "vae.sess.run(tf.assign(vae.C, 0.05))\n",
    "adv_recon = []\n",
    "adv_recon.append(original_reconstruction)\n",
    "points = [point]\n",
    "\n",
    "titles = ['Original','Target','Original Reconstruction',\n",
    "          'Adversarial image','Attack','Attacked Reconstruction']\n",
    "attack_iterations = 0\n",
    "for i in range(1000):\n",
    "    vae.sess.run(vae.adversarial_optimizer,feed_dict=feed_dict)\n",
    "    \n",
    "    adv_cost = vae.sess.run(vae.target_cost, feed_dict=feed_dict)\n",
    "    \n",
    "    reconstruction = vae.reconstruct(original)[0]\n",
    "    attack = vae.sess.run(vae.attack)\n",
    "    adversarial_image = vae.sess.run(vae.adversarial_x,feed_dict)\n",
    "    \n",
    "    point = (distance(attack),distance(target[0]-reconstruction))\n",
    "    points.append(point)\n",
    "    \n",
    "    adv_recon.append(reconstruction)\n",
    "    attack_iterations += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        series = [original,target,original_reconstruction,\n",
    "                 adversarial_image,attack,reconstruction]\n",
    "        suptitle='After %d attack iterations' % attack_iterations\n",
    "        plot_images(series=series,titles=titles,suptitle=suptitle, img_dim=(32,32,3))\n",
    "        plt.show()\n",
    "\n",
    "        print(np.mean(adv_cost))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.axvline(dist_target_original)\n",
    "plt.axhline(dist_target_rec_target,color='orange')\n",
    "plt.axhline(dist_target_rec_original,color='green')\n",
    "x,y=list(zip(*points))\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gif Maker\n",
    "\n",
    "ta com algum problema, n√£o sei direito ainda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML, Image\n",
    "\n",
    "fig = plt.figure()\n",
    "im = plt.imshow(adv_recon[0].reshape(32,32,3),animated=True, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "i = 0 \n",
    "def updatefig(i):\n",
    "    i += 1\n",
    "    im.set_array(adv_recon[i*2].reshape(32,32,3))\n",
    "    return im,\n",
    "\n",
    "ani = animation.FuncAnimation(fig, updatefig, interval=100, blit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(adv_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "\n",
    "def f(x, y):\n",
    "    return np.sin(x) + np.cos(y)\n",
    "\n",
    "x = np.linspace(0, 2 * np.pi, 120)\n",
    "y = np.linspace(0, 2 * np.pi, 100).reshape(-1, 1)\n",
    "\n",
    "im = plt.imshow(f(x, y), animated=True)\n",
    "\n",
    "\n",
    "def updatefig(*args):\n",
    "    global x, y\n",
    "    x += np.pi / 15.\n",
    "    y += np.pi / 20.\n",
    "    im.set_array(f(x, y))\n",
    "    return im,\n",
    "\n",
    "ani = animation.FuncAnimation(fig, updatefig, interval=50, blit=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_video(vid_file, frames):\n",
    "    height, width, layers = frames[0].shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "    video = cv2.VideoWriter(vid_file, fourcc, 30, (width,height))\n",
    "\n",
    "    for frame in frames:\n",
    "        r,g,b = cv2.split(frame)\n",
    "        bgr_img = cv2.merge([b,g,r])\n",
    "        #video.write(bgr_img)\n",
    "        video.write(frame)\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adv_recon = [im.reshape(32,32,3) for im in adv_recon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adv_recon = [im.transpose(1,2,0) for im in adv_recon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "file = 'images/test.avi'\n",
    "save_video(file, adv_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network_architecture = \\\n",
    "    dict(n_hidden_recog_1=500, # 1st layer encoder neurons\n",
    "         n_hidden_recog_2=500, # 2nd layer encoder neurons\n",
    "         n_hidden_gener_1=500, # 1st layer decoder neurons\n",
    "         n_hidden_gener_2=500, # 2nd layer decoder neurons\n",
    "         n_input=784, # MNIST data input (img shape: 28*28)\n",
    "         n_z=2)  # dimensionality of latent space\n",
    "\n",
    "vae_2d = train(network_architecture, training_epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_sample, y_sample = mnist.test.next_batch(5000)\n",
    "z_mu = vae_2d.transform(x_sample)\n",
    "plt.figure(figsize=(8, 6)) \n",
    "plt.scatter(z_mu[:, 0], z_mu[:, 1], c=np.argmax(y_sample, 1))\n",
    "plt.colorbar()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
