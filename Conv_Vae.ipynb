{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import datasets\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58606, 3072)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading SVHN training images...\n",
      "loading SVHN test images...\n",
      "58606\n",
      "14651\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = datasets.load_SVHN()\n",
    "\n",
    "\n",
    "#split validation set\n",
    "train_set, val_set = datasets.split_validation(train_set, percentage=0.2)\n",
    "\n",
    "train_x, train_y = train_set\n",
    "val_x, val_y = val_set\n",
    "\n",
    "\n",
    "\n",
    "n_samples = len(train_x)\n",
    "n_val = len(val_x)\n",
    "print(n_samples)\n",
    "print(n_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    \"\"\" Xavier initialization of network weights\"\"\"\n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval=low, maxval=high, \n",
    "                             dtype=tf.float32)\n",
    "\n",
    "sigmoid = tf.nn.sigmoid\n",
    "elu = tf.nn.elu\n",
    "\n",
    "def bias(size, zero=False):\n",
    "    if zero:\n",
    "        return tf.Variable(tf.zeros([size], dtype=tf.float32))\n",
    "    else:\n",
    "        return tf.Variable(tf.random_normal([size], stddev=0.1))\n",
    "\n",
    "def conv(tensor, kernel_dims):\n",
    "    ksize, n_in, n_out = kernel_dims\n",
    "    kernels = tf.Variable(tf.random_normal([ksize, ksize, n_in, n_out],  stddev=0.1))\n",
    "    return tf.nn.conv2d(tensor,kernels, strides=[1, 2, 2, 1], padding='SAME') + bias(n_out)\n",
    "\n",
    "def deconv(tensor, kernel_dims, out_dim, stride=[1,2,2,1]):\n",
    "    ksize, n_out, n_in = kernel_dims\n",
    "    kernels = tf.Variable(tf.random_normal([ksize, ksize, n_out, n_in], stddev=0.1))\n",
    "    out_dim = tf.stack(out_dim)\n",
    "    return tf.nn.conv2d_transpose(tensor, kernels, out_dim, strides=stride, padding='SAME') + bias(n_out)\n",
    "\n",
    "def dense(tensor, in_size, out_size):\n",
    "    weights = tf.Variable(xavier_init(in_size, out_size))\n",
    "    return tf.matmul(tensor, weights) + bias(out_size)\n",
    "    \n",
    "\n",
    "def clip(tensor, _max=None, _min=None):\n",
    "    return tf.clip_by_value(tensor, clip_value_min=_min, clip_value_max=_max)\n",
    "\n",
    "def clippedAdam(loss, weights):\n",
    "    reconstr_grads = tf.gradients(loss, w)\n",
    "    \n",
    "    # Clip gradient\n",
    "    def ClipIfNotNone(grad):\n",
    "        if grad is None:\n",
    "            return grad\n",
    "        return tf.clip_by_value(grad, -1, 1)\n",
    "    \n",
    "    reconstr_grads, _ = tf.clip_by_global_norm(reconstr_grads, 5.)\n",
    "    reconstr_grads = [ClipIfNotNone(grad) for grad in reconstr_grads]\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    return optimizer.apply_gradients(zip(reconstr_grads, w))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model\n",
    "n_input = 32*32*3\n",
    "n1 = 32\n",
    "n2 = 64\n",
    "n3 = 128\n",
    "n_z = 100\n",
    "batch_size = 100\n",
    "ksize = 5\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_input],'X')\n",
    "n_ex = tf.shape(x)[0] #number of examples\n",
    "\n",
    "#attack layer starts out at zero\n",
    "attack = tf.Variable(tf.zeros([n_input], dtype=tf.float32),name='Attack')\n",
    "adv_x = clip(x + attack, _min=0, _max=1)\n",
    "\n",
    "adv_im = tf.reshape(adv_x, shape=[-1, 32, 32, 3])\n",
    "\n",
    "#encoder\n",
    "conv_enc1  = elu(conv(adv_im, [ksize, 3, n1]))\n",
    "conv_enc2  = elu(conv(conv_enc1, [ksize, n1, n2]))\n",
    "conv_enc3  = elu(conv(conv_enc2, [ksize, n2, n3]))\n",
    "flat = tf.reshape(conv_enc3, [-1, 4*4*n3])\n",
    "dens_enc4 = elu(dense(flat, 4*4*n3, 512))\n",
    "\n",
    "z_mean = dense(dens_enc4, 512, n_z)\n",
    "z_log_var = dense(dens_enc4, 512, n_z)\n",
    "\n",
    "\n",
    "#sample the latent variables\n",
    "eps = tf.random_normal((batch_size, n_z), 0, 1, dtype=tf.float32)\n",
    "z = z_mean + eps*tf.sqrt(tf.exp(z_log_var)) #sampling z from normal\n",
    "\n",
    "\n",
    "#generator\n",
    "from_z = elu(dense(z, n_z, 512))\n",
    "im_z = tf.reshape(from_z, [-1,4,4,32],name='im_z')\n",
    "deconv_dec1 = elu(deconv(im_z, [ksize, n3, 32], out_dim=[n_ex, 8, 8, n3]))\n",
    "deconv_dec2 = elu(deconv(deconv_dec1, [ksize, n2, n3], out_dim=[n_ex, 16, 16, n2]))\n",
    "deconv_dec3 = elu(deconv(deconv_dec2, [ksize, n1, n2], out_dim=[n_ex, 32, 32, n1]))\n",
    "\n",
    "out_mu = deconv(deconv_dec3, [ksize, 3, n1], out_dim=[n_ex, 32, 32, 3], stride=[1,1,1,1])\n",
    "out_log_var = deconv(deconv_dec3, [ksize, 3, n1], out_dim=[n_ex, 32, 32, 3], stride=[1,1,1,1])\n",
    "\n",
    "reconstr_mean = sigmoid(tf.reshape(out_mu, [-1,32*32*3]))\n",
    "reconstr_log_var = tf.reshape(out_log_var, [-1,32*32*3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Getting model variables and attack variable\n",
    "all_vars = sess.graph.get_collection('trainable_variables')\n",
    "w = [var for var in all_vars if 'Attack' not in var.name]\n",
    "\n",
    "c = - 0.5 * math.log(2 * math.pi)\n",
    "def log_normal2(x, mean, log_var, eps=1e-8):\n",
    "    return c - log_var / 2 - tf.pow(x - mean, 2) / (2 * tf.exp(log_var) + eps)\n",
    "\n",
    "def kl_divergence(mean1, log_var1, mean2, log_var2, eps=1e-8):\n",
    "    mean_term = 0.5 * (tf.exp(log_var1) + tf.pow(mean1 - mean2, 2)) \\\n",
    "        / (tf.exp(log_var2) + eps)\n",
    "    return mean_term + 0.5 * log_var2 - 0.5 * log_var1 - 0.5\n",
    "\n",
    "#loss functions\n",
    "log_px_given_z = tf.reduce_sum(log_normal2(x, reconstr_mean, reconstr_log_var),axis=1)\n",
    "kl_term = tf.reduce_sum(kl_divergence(z_mean, z_log_var, 0., 1.), axis=1)\n",
    "cost = -tf.reduce_mean(-kl_term + log_px_given_z)\n",
    "\n",
    "#using Adam optimizer\n",
    "optimizer = clippedAdam(cost, w)\n",
    "\n",
    "\n",
    "#Adversarial Cost\n",
    "Target = tf.placeholder(tf.float32, [None, n_z])\n",
    "C = tf.placeholder(tf.float32,name='C')\n",
    "target_cost = tf.reduce_mean(tf.square(Target - z_mean)) + C*tf.nn.l2_loss(attack)\n",
    "\n",
    "target_opt = tf.train.AdamOptimizer(learning_rate=0.001).minimize(target_cost,var_list=attack)\n",
    "\n",
    "init_attack = tf.variables_initializer([attack])\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partial_fit(X):\n",
    "    \"\"\"Train model based on mini-batch of input data.\n",
    "\n",
    "    Return cost of mini-batch.\n",
    "    \"\"\"\n",
    "    opt, c = sess.run((optimizer, cost), \n",
    "                              feed_dict={x: X})\n",
    "    return c\n",
    "\n",
    "def get_cost(X):\n",
    "    c = sess.run((cost), feed_dict={x: X})\n",
    "    return c\n",
    "\n",
    "\n",
    "def reconstruct(X):\n",
    "    \"\"\" Use VAE to reconstruct given data. \"\"\"\n",
    "    return sess.run(out_mean, feed_dict={x: X})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0001, COST: 5138379213483.89160, VAL_COST: 19409.82872\n",
      "EPOCH: 0011, COST: 19206.62940, VAL_COST: 19166.33289\n",
      "EPOCH: 0021, COST: 19198.95537, VAL_COST: 19144.34112\n",
      "EPOCH: 0031, COST: 19199.66780, VAL_COST: 19170.33343\n",
      "EPOCH: 0041, COST: 19198.17456, VAL_COST: 19127.20752\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 100\n",
    "display_step = 1\n",
    "\n",
    "sess.run(init)\n",
    "# Training cycle\n",
    "val_losses = []\n",
    "losses = []\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    avg_val = 0.\n",
    "    # Loop over all batches\n",
    "    for batch_xs, ys in datasets.batches(train_x, train_y, batch_size=batch_size):\n",
    "        if len(batch_xs) != batch_size:\n",
    "            continue\n",
    "        # Fit training using batch data\n",
    "        c = partial_fit(batch_xs)\n",
    "        # Compute average loss\n",
    "        avg_cost += c / n_samples * batch_size\n",
    "\n",
    "    #validation set costs\n",
    "    for batch_xs, ys in datasets.batches(val_x, val_y, batch_size=batch_size):\n",
    "        if len(batch_xs) != batch_size:\n",
    "            continue\n",
    "        # Fit training using batch data\n",
    "        c = get_cost(batch_xs)\n",
    "        # Compute average loss\n",
    "        avg_val += c / n_val * batch_size\n",
    "    \n",
    "    val_losses.append(avg_val)\n",
    "    losses.append(avg_cost)\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print (\"EPOCH: %04d, COST: %03.5f, VAL_COST: %03.5f\" % (epoch+1, avg_cost, avg_val))\n",
    "print (\"EPOCH: %04d, COST: %03.5f, VAL_COST: %03.5f\" % (epoch+1, avg_cost, avg_val))\n",
    "plt.plot(losses)\n",
    "_= plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = datasets.batches(train_x, train_y, batch_size=batch_size)\n",
    "x_sample, _ = next(batch)\n",
    "x_reconstruct = reconstruct(x_sample)\n",
    "plt.figure(figsize=(8, 12))\n",
    "for i in range(5):\n",
    "    plt.subplot(5, 2, 2*i + 1)\n",
    "    plt.imshow(x_sample[i].reshape(32, 32, 3), vmin=0, vmax=1, cmap=\"gray\")\n",
    "    plt.title(\"Test input\")\n",
    "    plt.colorbar()\n",
    "    plt.subplot(5, 2, 2*i + 2)\n",
    "    plt.imshow(x_reconstruct[i].reshape(32, 32, 3), vmin=0, vmax=1, cmap=\"gray\")\n",
    "    plt.title(\"Reconstruction\")\n",
    "    plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(images=[],titles=[],suptitle=''):\n",
    "    num_colums = 3\n",
    "    num_imgs = len(images)\n",
    "    num_rows = 2\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.suptitle(suptitle,fontsize=16)\n",
    "    for i in range(num_imgs):\n",
    "        plt.subplot(num_rows, num_colums, i+1)\n",
    "        plt.imshow(images[i].reshape(32, 32, 3),vmin=0,vmax=1,cmap='gray')\n",
    "        plt.title(titles[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def choose_original_target(x):\n",
    "    n = len(x)\n",
    "    i_original = np.random.choice(n)\n",
    "    i_target = np.random.choice(n)\n",
    "    original, target = x[i_original], x[i_target]\n",
    "    return original, target\n",
    "\n",
    "def stack(array, stack_size=100):\n",
    "    return np.array([array for i in range(stack_size)])\n",
    "\n",
    "def get_z(X):\n",
    "    z = sess.run((z_mean), feed_dict={x: X})\n",
    "    return z\n",
    "\n",
    "def reset_attack():\n",
    "    z = sess.run(init_attack)\n",
    "\n",
    "def fit_attack(feed_dict):\n",
    "    target_loss, _ = sess.run((target_cost, target_opt), feed_dict=feed_dict)\n",
    "    return target_loss\n",
    "\n",
    "def get_vars(X):\n",
    "    return sess.run((out_mean, attack, adv_x), feed_dict={x: X})\n",
    "    \n",
    "def distance(a, b):\n",
    "    return np.mean(np.linalg.norm(a-b, axis=1))\n",
    "\n",
    "def norm(a):\n",
    "    return np.linalg.norm(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig, targ = choose_original_target(train_x)\n",
    "orig = stack(orig, stack_size=100)\n",
    "targ = stack(targ, stack_size=100)\n",
    "\n",
    "reset_attack()\n",
    "target_z = np.array([get_z(targ)[0]])\n",
    "orig_recon = reconstruct(orig)\n",
    "targ_recon = reconstruct(targ)\n",
    "\n",
    "targ_2_orig = distance(orig_recon, targ_recon)\n",
    "targ_2_recon = distance(targ, targ_recon)\n",
    "origrec_2_target = distance(orig_recon, targ)\n",
    "\n",
    "\n",
    "plot_images(images=[orig[0],targ[0]],titles=['original','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Vaores de C para explorar\n",
    "Cs = np.logspace(10, -20, 100, base = 2, dtype = np.float32)\n",
    "\n",
    "points = []\n",
    "titles = ['Original','Target','Original Reconstruction',\n",
    "              'Adversarial image','Attack','Attacked Reconstruction']\n",
    "for i, c in enumerate(Cs):\n",
    "    reset_attack()\n",
    "    feed_dict = {x:orig, Target:target_z, C:c}\n",
    "    print('%d C: %f'%(i+1, c))\n",
    "    for i in range(2000):\n",
    "        loss = fit_attack(feed_dict)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(np.mean(loss))\n",
    "    \n",
    "    recon, att, ad_img = get_vars(orig)\n",
    "    \n",
    "    size_attack = norm(att)\n",
    "    dist_recon = distance(recon,targ)\n",
    "    \n",
    "    point = (size_attack,dist_recon)\n",
    "    points.append(point)\n",
    "    images = [orig[0], targ[0], orig_recon[0], ad_img[0], att, recon[0]]\n",
    "    suptitle='C: %f ' % c\n",
    "    plot_images(images=images,titles=titles,suptitle=suptitle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(points):\n",
    "    plt.figure()\n",
    "    plt.axvline(targ_2_orig, color='cyan', linewidth=2, label=\"Original - Target\")\n",
    "    plt.axhline(targ_2_recon, color='red', linewidth=2, label=\"Target rec. - Target\")\n",
    "    plt.axhline(origrec_2_target, color='DarkOrange', linewidth=2, label=\"Original rec. - Target\")\n",
    "    x,y=list(zip(*points))\n",
    "    plt.scatter(x,y)\n",
    "    plt.ylabel(\"Adversarial rec. - Target\")\n",
    "    plt.xlabel(\"Distortion\")\n",
    "    plt.legend()\n",
    "    \n",
    "plot_results(points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
